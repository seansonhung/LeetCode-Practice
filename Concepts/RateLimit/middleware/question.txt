The exercise is designed to take about 90-120 minutes for someone comfortable with nodejs and express.
We provide you with a basic "Hello World" api boilerplate in a nodeJS and express implementation (v16.13.0 supporting almost all ES2015 features), as well as a basic integration test that calls to that existing endpoint. To support the testing we use mocha and chai. Note that all your tests must be in the /specs folder to run.
The boilerplate is meant to accelerate you but you can change it and the project setup as you see fit, you have access to a full terminal and are allowed to install any additional npm packages or scripts. The pre-installed packages should allow you to complete the full exercise but you should feel free to add anything else to design your solution as you see fit.
This exercise represents a MVP / POC feature, that we will expand upon further in the following interview stages.
There are no hidden tests or code that will automatically score your exercise. It will be manually reviewed and graded by our team's engineers, with expectations being adjusted based to your seniority.
Implement your solution as if you were developing code to be deployed and maintained by you and your team as best as you can given the limited time frame, the sandbox environment limitations and the requirements asked, and document any compromises, gaps or possible follow-ups in a README.md file.
Given you won't have access to the exercise after you submit it we recommend you keep a copy of your code so you can remember it for the next interview stages.
Additional Guidance
There is a 2 hour time limit for this question, unless it is refreshed. We recommend that you submit your code and refresh once an hour to ensure your work is saved and a timeout error is avoided
Running
Make sure all packages are installed by running npm install first
You can run your api with npm start (stop a running application with ctrl + c)
You can run all your tests with npm test
To call your api, open a separate terminal and use curl, for example:
curl http://localhost:3000
curl -X POST http://localhost:3000/ -H 'content-type: application/json' -d '{"name":"john smith"}'
Exercise
To protect our customers and our APIs, we use a rate limits mechanism.

Your task today will be that of implementing a small standalone service to provide that capability to our other services that clients directly use. This service will only be responsible for keeping track of usage and limits for a given resource, won't be accessible from the public internet and will not expose any customer facing endpoints.

Token bucket algorithm
The way we express rate limiting is in the form of Burst limit (higher) and sustained limit (lower), for instance you could have a rate limit defined for the route template GET users/:id of 100 req burst and 50 req/min sustained.

This means you can accumulate requests' "credit", or tokens, up to a maximum of 100, but they only refresh at a rate of a total of 50 per minute. Every time you check to allow a request for a certain route template, you deduct one token. If you run out of tokens the request should be rejected, until tokens replenish at the defined sustained rate.

An algorithm that seems fit for this approach is what's called Token Bucket. Feel free to use it in your solution or not, as long as the requirements presented below are met.

Requirements
Please implement a small service that:

Loads the configuration represented in config.json every time it starts. In there you will find an array at rateLimitsPerEndpoint with a configuration for each route template the service should provide a rate limit for:
endpoint: route template being limited, acts as a static key provided by the caller to check and consume its request tokens (i.e. the key used by the caller will always be "GET /user/:id" even if the original request was to /user/12345)
burst: the number of burst requests allowed
sustained: the number of sustained requested per minute.
Exposes a single endpoint at /take/, which receives the route template to check the rate limit for from the caller.
This service is called by other client facing services (i.e. as part of their middleware pipeline) to determine if a given request should be accepted or not for the specified route template:
If the limit for the specified route is not exceeded, the endpoint should consume one token and return a response with the number of remaining tokens and confirmation the request should be accepted.
If the limit for the specified route has already been met (empty bucket), the endpointâ€™s response should have a result of 0 remaining tokens and inform the consumer the request should be rejected.
The bucket for each endpoint starts containing a burst number of tokens, and refills with a sustained number of tokens per minute up to a max of burst number of tokens.
The refilling of tokens should happen as soon as possible, i.e. a sustained rate of 120 req/min should make available 1 token every 0.5 seconds.
Assumptions
The configuration json will always fit in memory.
Assume this service will run in a single instance.
Service state can be kept in memory only (can be lost if service crashes, etc).
The service runs in our internal network, with no exposure to the internet, and without any need to identify or authenticate the caller.
Good luck!



app.post('/take', (req, res) => {
  const { routeTemplate } = req.body;

  try {
    const checkLimitResult = checkRateLimit(routeTemplate);

    if (checkLimitResult.error) {
      return res.status(404).json({ message: checkLimitResult.error });
    }

    if (checkLimitResult.limitHit) {
      return res.status(429).json({ message: "Limit for the route has been met, request rejected", remainingTokens: checkLimitResult.remainingTokens });
    }

    return res.status(200).json({ message: "The request should be accepted", remainingTokens: checkLimitResult.remainingTokens });
  } catch (err) {
    console.error("Error in /take endpoint:", err);
    return res.status(500).json({ message: "Internal server error" });
  }
});